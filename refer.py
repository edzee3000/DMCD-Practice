import sys
import pickle
import os
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import IterableDataset, DataLoader
from tqdm import tqdm
from Models import *
import warnings
import gc  # å¯¼å…¥åƒåœ¾å›æ”¶æ¨¡å—
warnings.filterwarnings('ignore')


# ====================== 1. æ ¸å¿ƒé…ç½®ï¼ˆå¿…é¡»ä¸è®­ç»ƒä¸€è‡´ï¼‰ ======================
# è·¯å¾„é…ç½®
DATA_DIR = "./Data/UserData"
CACHE_DIR = "./Data/Data_Cache"
TEST_PATH = os.path.join(DATA_DIR, "subscribe_test.txt")  # ä½ çš„æµ‹è¯•é›†è·¯å¾„
MODEL_PATH = "./WideDeep_Epoch_5.pth"  # è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡
SUBMISSION_PATH = "./Data/submission.csv"    # æœ€ç»ˆè¾“å‡ºæ–‡ä»¶ï¼ˆåŒ¹é…ç›®æ ‡æ ¼å¼ï¼‰

# æ¨¡å‹å‚æ•°ï¼ˆå’Œè®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼ï¼‰
WIDE_DIM = 8               # Wideéƒ¨åˆ†ç»´åº¦
DEEP_NUMERIC_DIM = 8       # Deepæ•°å€¼ç‰¹å¾ç»´åº¦
HIDDEN_DIMS = [128, 64]    # Deepéƒ¨åˆ†éšè—å±‚

# æ¨ç†å‚æ•°
STREAM_BATCH_SIZE = 524288  # æµå¼è¯»å–æµ‹è¯•é›†æ‰¹æ¬¡ï¼ˆå‡å°‘å†…å­˜ï¼‰
# STREAM_BATCH_SIZE = 1048576  # æµå¼è¯»å–æµ‹è¯•é›†æ‰¹æ¬¡ï¼ˆå‡å°‘å†…å­˜ï¼‰
# STREAM_BATCH_SIZE = 2097152  # æµå¼è¯»å–æµ‹è¯•é›†æ‰¹æ¬¡ï¼ˆå‡å°‘å†…å­˜ï¼‰
INFER_BATCH_SIZE = 8192     # æ¨¡å‹æ¨ç†æ‰¹æ¬¡ï¼ˆæ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼‰
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ====================== 2. åŠ è½½ç¼“å­˜èµ„æºï¼ˆç‰¹å¾å­—å…¸/è¯æ±‡è¡¨ï¼‰ ======================
def load_cache():
    """åŠ è½½è®­ç»ƒæ—¶ç”Ÿæˆçš„ç¼“å­˜æ–‡ä»¶ï¼Œä¿è¯ç‰¹å¾ä¸€è‡´"""
    cache_files = {
        "a_user": "a_user_dict.pkl",
        "b_user": "b_user_dict.pkl",
        "interact": "interact_dict.pkl",
        "subscribe": "subscribe_dict.pkl",
        "keyword": "keyword_dict.pkl",
        "vocab": "vocab_dict.pkl"
    }
    cache = {}
    for name, filename in cache_files.items():
        path = os.path.join(CACHE_DIR, filename)
        if not os.path.exists(path):
            raise FileNotFoundError(f"ç¼“å­˜æ–‡ä»¶ç¼ºå¤±ï¼š{path}")
        with open(path, "rb") as f:
            cache[name] = pickle.load(f)
        print(f"âœ… {name} ç¼“å­˜åŠ è½½å®Œæˆ")
    return cache

# ====================== 3. æµ‹è¯•é›†æ•°æ®é›†ï¼ˆæµå¼è¯»å–+ç‰¹å¾æå–ï¼‰ ======================
class TestDataset(IterableDataset):
    def __init__(self, test_path, cache):
        self.test_path = test_path
        self.cache = cache
        # æå–ç‰¹å¾å­—å…¸
        self.a_user = cache["a_user"]
        self.b_user = cache["b_user"]
        self.interact = cache["interact"]
        self.subscribe = cache["subscribe"]
        self.keyword = cache["keyword"]
        self.vocab = cache["vocab"]

    def _parse_line(self, line):
        """è§£ææµ‹è¯•é›†è¡Œï¼šA_id\tB_id\t0\ttimestamp â†’ è¿”å›A_id, B_id"""
        line = line.strip()
        if not line:
            return None, None
        parts = line.split("\t")
        if len(parts) < 2:
            return None, None
        return parts[0], parts[1]

    def _extract_feat(self, A_id, B_id):
        """æå–ä¸è®­ç»ƒå®Œå…¨ä¸€è‡´çš„ç‰¹å¾ï¼ˆWide+Deepï¼‰"""
        # ---------------- Wideç‰¹å¾ï¼ˆ8ç»´ï¼‰ ----------------
        wide = np.zeros(WIDE_DIM, dtype=np.float32)
        # 1. Aç”¨æˆ·æ€§åˆ«
        a_gender = self.a_user.get(A_id, {}).get("gender", 0)
        wide[0] = self.vocab["gender_vocab"].get(str(a_gender), 0)
        # 2. Bç”¨æˆ·ä¸€çº§åˆ†ç±»
        b_cate1 = self.b_user.get(B_id, {}).get("cate_1", "0")
        wide[1] = self.vocab["cate_vocab"].get(b_cate1, 0)
        # 3. å…³é”®è¯äº¤é›†å æ¯”
        a_kw = set(self.keyword.get(A_id, {}).keys())
        b_kw = set(self.b_user.get(B_id, {}).get("keyword_ids", []))
        wide[2] = len(a_kw & b_kw) / max(len(b_kw), 1)
        # 4. Aæ˜¯å¦å…³æ³¨B
        wide[3] = 1.0 if B_id in self.subscribe.get(A_id, set()) else 0.0
        # 5. äº¤äº’æ¬¡æ•°å½’ä¸€åŒ–
        inter = self.interact.get((A_id, B_id), {"@num":0, "forward_num":0, "comment_num":0})
        total_inter = inter["@num"] + inter["forward_num"] + inter["comment_num"]
        wide[4] = min(total_inter / 100, 1.0)
        # 6. Aç”¨æˆ·æ ‡ç­¾æ•°é‡å½’ä¸€åŒ–
        a_tag_num = len(self.a_user.get(A_id, {}).get("tag_ids", []))
        wide[5] = min(a_tag_num / 20, 1.0)
        # 7. Bç”¨æˆ·å…³é”®è¯æ•°é‡å½’ä¸€åŒ–
        b_kw_num = len(self.b_user.get(B_id, {}).get("keyword_ids", []))
        wide[6] = min(b_kw_num / 50, 1.0)
        # 8. Aç”¨æˆ·å‘å¸–æ•°å½’ä¸€åŒ–
        a_post_num = self.a_user.get(A_id, {}).get("post_num", 0)
        wide[7] = min(a_post_num / 1000, 1.0)

        # ---------------- Deepç‰¹å¾ï¼ˆ16ç»´ï¼‰ ----------------
        deep = np.zeros(16, dtype=np.float32)
        # æ•°å€¼ç‰¹å¾ï¼ˆå‰8ç»´ï¼‰
        deep[0] = np.log1p(a_post_num)
        deep[1] = np.log1p(inter["@num"])
        deep[2] = np.log1p(inter["forward_num"])
        deep[3] = np.log1p(inter["comment_num"])
        deep[4] = np.log1p(sum(self.keyword.get(A_id, {}).values()))
        deep[5] = np.log1p(a_tag_num)
        deep[6] = np.log1p(b_kw_num)
        deep[7] = np.log1p(total_inter)
        # ç¦»æ•£ç‰¹å¾ï¼ˆå8ç»´ï¼Œè½¬ä¸ºè¯æ±‡è¡¨IDï¼‰
        deep[8] = self.vocab["gender_vocab"].get(str(a_gender), 0)
        deep[9] = self.vocab["tag_vocab"].get(self.a_user.get(A_id, {}).get("tag_ids", ["0"])[0], 0)
        deep[10] = self.vocab["cate_vocab"].get(b_cate1, 0)
        deep[11] = self.vocab["cate_vocab"].get(self.b_user.get(B_id, {}).get("cate_2", "0"), 0)
        deep[12] = self.vocab["cate_vocab"].get(self.b_user.get(B_id, {}).get("cate_3", "0"), 0)
        deep[13] = self.vocab["cate_vocab"].get(self.b_user.get(B_id, {}).get("cate_4", "0"), 0)
        deep[14] = self.vocab["kw_vocab"].get(self.b_user.get(B_id, {}).get("keyword_ids", ["0"])[0], 0)
        deep[15] = self.vocab["kw_vocab"].get(list(a_kw)[0] if a_kw else "0", 0)

        # ç‰¹å¾å½’ä¸€åŒ–+å¼‚å¸¸å€¼å¤„ç†
        wide = np.nan_to_num(wide, 0.0)
        deep[:8] = (deep[:8] - deep[:8].min()) / (deep[:8].max() - deep[:8].min() + 1e-8)
        deep = np.nan_to_num(deep, 0.0)
        return wide, deep

    def __iter__(self):
        """æµå¼è¿­ä»£ï¼šè¿”å› (wide_feat, deep_feat, A_id, B_id)"""
        with open(self.test_path, "r", encoding="utf-8", errors="ignore") as f:
            batch = []
            for line in f:
                A_id, B_id = self._parse_line(line)
                if not A_id or not B_id:
                    continue
                wide, deep = self._extract_feat(A_id, B_id)
                batch.append((wide, deep, A_id, B_id))
                # æ‰¹æ¬¡æ»¡åˆ™è¿”å›
                if len(batch) >= STREAM_BATCH_SIZE:
                    for item in batch:
                        yield item
                    batch = []
            # å¤„ç†æœ€åä¸€æ‰¹
            for item in batch:
                yield item



# ====================== 5. æ ¸å¿ƒæ¨ç†é€»è¾‘ï¼ˆæ¨¡å‹è¾“å‡ºâ†’ç›®æ ‡æ ¼å¼ï¼‰ ======================
def main():
    # 1. åŠ è½½ç¼“å­˜å’Œæ¨¡å‹
    cache = load_cache()
    vocab = cache["vocab"]  # ä»ç¼“å­˜ä¸­è·å–è¯æ±‡è¡¨
    EMBED_CONFIG = [
        (len(vocab["gender_vocab"]), 4),
        (len(vocab["tag_vocab"]), 8),
        (len(vocab["cate_vocab"]), 8),
        (len(vocab["cate_vocab"]), 8),
        (len(vocab["cate_vocab"]), 8),
        (len(vocab["cate_vocab"]), 8),
        (len(vocab["kw_vocab"]), 8),
        (len(vocab["kw_vocab"]), 8)
    ]
    # åˆå§‹åŒ–æ¨¡å‹
    model = WideDeep(
        wide_dim=WIDE_DIM,
        deep_numeric_dim=DEEP_NUMERIC_DIM,
        embed_config=EMBED_CONFIG,
        hidden_dims=HIDDEN_DIMS
    ).to(DEVICE)
    # åŠ è½½æƒé‡ï¼ˆå…¼å®¹CPU/GPUï¼‰
    state_dict = torch.load(MODEL_PATH, map_location=DEVICE)
    model.load_state_dict(state_dict)
    model.eval()  # æ¨ç†æ¨¡å¼ï¼ˆå…³é—­Dropoutï¼‰
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œè®¾å¤‡ï¼š{DEVICE}")

    # 2. åˆå§‹åŒ–æµ‹è¯•é›†å’ŒDataLoader
    test_dataset = TestDataset(TEST_PATH, cache)
    test_loader = DataLoader(
        test_dataset,
        batch_size=INFER_BATCH_SIZE,
        num_workers=0,
        pin_memory=True
    )

    # 3. æ¨ç†ï¼šæ”¶é›†æ¯ä¸ªç”¨æˆ·Açš„å€™é€‰BåŠå¾—åˆ†
    user_candidates = {}  # key: A_id, value: [(B_id, score), ...]
    total_processed = 0  # æ–°å¢ï¼šç”¨äºç»Ÿè®¡å·²å¤„ç†çš„æ ·æœ¬æ€»æ•°
    with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦ï¼ŒèŠ‚çœå†…å­˜
        pbar = tqdm(test_loader, desc="æ¨ç†ä¸­", unit="batch")
        for batch in pbar:
            wide_feat, deep_feat, A_ids, B_ids = batch
            batch_size = len(A_ids)  # å½“å‰æ‰¹æ¬¡æ ·æœ¬æ•°
            # æ•°æ®è½¬tensorå¹¶ç§»åˆ°è®¾å¤‡
            wide_feat = torch.tensor(np.stack(wide_feat)).to(DEVICE)
            deep_feat = torch.tensor(np.stack(deep_feat)).to(DEVICE)
            # æ¨¡å‹é¢„æµ‹ï¼ˆlogitsâ†’æ¦‚ç‡ï¼‰
            logits = model(wide_feat, deep_feat)
            scores = torch.sigmoid(logits).cpu().numpy()  # è½¬ä¸º0-1çš„å…³æ³¨æ¦‚ç‡

            # æŒ‰ç”¨æˆ·Aåˆ†ç»„å­˜å‚¨
            for A_id, B_id, score in zip(A_ids, B_ids, scores):
                if A_id not in user_candidates:
                    user_candidates[A_id] = []
                user_candidates[A_id].append((B_id, score))
            
            # # å…³é”®ä¼˜åŒ–ï¼šåˆ é™¤å½“å‰æ‰¹æ¬¡å˜é‡ï¼Œé‡Šæ”¾å†…å­˜
            # del wide_feat, deep_feat, logits, scores, A_ids, B_ids
            # # torch.cuda.empty_cache()  # é‡Šæ”¾GPUç¼“å­˜ï¼ˆå¦‚æœç”¨GPUï¼‰
            # gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
            
            # ç´¯è®¡æ ·æœ¬æ•°ï¼Œå¹¶æ›´æ–°è¿›åº¦æ¡çŠ¶æ€
            total_processed += batch_size
            # å°†å·²å¤„ç†æ ·æœ¬æ•°æ·»åŠ åˆ°è¿›åº¦æ¡çš„åç¼€ä¿¡æ¯ä¸­ï¼ˆæ˜¾ç¤ºåœ¨[]å†…ï¼‰
            pbar.set_postfix(samples=f"{total_processed}")
    # ä¿å­˜user_candidatesåˆ°ç¼“å­˜  é˜²æ­¢åé¢ä»£ç å‡ºé”™
    CANDIDATES_CACHE_PATH = os.path.join(CACHE_DIR, "user_candidates.pkl")
    os.makedirs(os.path.dirname(CANDIDATES_CACHE_PATH), exist_ok=True)
    with open(CANDIDATES_CACHE_PATH, "wb") as f:
        pickle.dump(user_candidates, f, protocol=pickle.HIGHEST_PROTOCOL)
    print(f"âœ… user_candidateså·²ä¿å­˜åˆ°ï¼š{CANDIDATES_CACHE_PATH}")
    # ä»ç¼“å­˜ä¸­é‡æ–°è¯»å–ï¼Œæ›¿æ¢åŸæœ‰å˜é‡
    with open(CANDIDATES_CACHE_PATH, "rb") as f:
        user_candidates = pickle.load(f)
    print(f"âœ… å·²ä»ç¼“å­˜åŠ è½½user_candidatesï¼Œå…±åŒ…å« {len(user_candidates)} ä¸ªç”¨æˆ·")

    # 4. ç”Ÿæˆç›®æ ‡æ ¼å¼çš„æäº¤æ–‡ä»¶
    with open(SUBMISSION_PATH, "w", encoding="utf-8") as f:
        # å†™å…¥è¡¨å¤´ï¼ˆåŒ¹é…ç¤ºä¾‹ï¼šid,clicksï¼‰
        f.write("id,clicks\n")
        # éå†æ¯ä¸ªç”¨æˆ·ï¼Œç”ŸæˆTOP3æ¨è
        for A_id in tqdm(user_candidates.keys(), desc="ç”Ÿæˆç»“æœ"):
            # æŒ‰å¾—åˆ†é™åºæ’åº â†’ å»é‡ â†’ å–å‰3
            candidates = sorted(user_candidates[A_id], key=lambda x: x[1], reverse=True)
            unique_B = []
            seen = set()
            for B_id, _ in candidates:
                if B_id not in seen:
                    seen.add(B_id)
                    unique_B.append(B_id)
                if len(unique_B) >= 3:
                    break
            # æ‹¼æ¥ä¸ºç©ºæ ¼åˆ†éš”çš„å­—ç¬¦ä¸²ï¼ˆåŒ¹é…ç¤ºä¾‹æ ¼å¼ï¼‰
            clicks = " ".join(unique_B) if unique_B else ""
            # å†™å…¥è¡Œï¼ˆid,clicksï¼‰
            f.write(f"{A_id},{clicks}\n")

    print(f"\nâœ… æ¨ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°ï¼š{SUBMISSION_PATH}")
    print(f"ğŸ“Š ç»Ÿè®¡ï¼šå…±å¤„ç† {len(user_candidates)} ä¸ªç”¨æˆ·")

if __name__ == "__main__":
    main()